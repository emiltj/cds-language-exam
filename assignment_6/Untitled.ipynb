{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Importing libraries ################################\n",
    "# system tools\n",
    "import os, sys, argparse\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "\n",
    "# pandas, numpy, gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from heapq import nlargest\n",
    "from scipy import stats\n",
    "\n",
    "# import my classifier utility functions - see the Github repo!\n",
    "import utils.classifier_utils as clf\n",
    "\n",
    "# Machine learning stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# tools from tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Embedding, \n",
    "                                     Flatten, GlobalMaxPool1D, Conv1D)\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Defining functions to be used in main ###############################\n",
    "def tokenize_X(X_train, X_test, num_words):\n",
    "    '''\n",
    "    Function that tokenizes X_train and X_test. \n",
    "    E.g. going from \"She's your new queen too.\" to [172, 8, 275, 103, 129] - each word now has an index pointing to a vocabulary list.\n",
    "    '''\n",
    "    \n",
    "    # Initialize tokenizer, using num_words as number of words\n",
    "    tokenizer = Tokenizer(num_words = num_words)\n",
    "    \n",
    "    # Fit tokenizer to training data\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # Make tokens into sequences\n",
    "    X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    # Return the sequenized tokens\n",
    "    return X_train_tokens, X_test_tokens, tokenizer\n",
    "\n",
    "def apply_padding(X_train, X_test, maxlen, placement):\n",
    "   \n",
    "    # Apply padding to train\n",
    "    X_train_pad = pad_sequences(X_train, \n",
    "                            padding = placement, # sequences can be padded \"pre\" or \"post\"\n",
    "                            maxlen = maxlen)\n",
    "\n",
    "    # Apply padding to test\n",
    "    X_test_pad = pad_sequences(X_test, \n",
    "                           padding = placement, \n",
    "                           maxlen = maxlen)\n",
    "    \n",
    "    # Return padded elements\n",
    "    return X_train_pad, X_test_pad\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    \"\"\" \n",
    "    A helper function to read in saved GloVe embeddings and create an embedding matrix\n",
    "    \n",
    "    filepath: path to GloVe embedding\n",
    "    word_index: indices from keras Tokenizer\n",
    "    embedding_dim: dimensions of keras embedding layer\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def plot_history(H, epoch, outpath):\n",
    "    \"\"\"\n",
    "    Utility function for plotting model history using matplotlib\n",
    "    \n",
    "    H: model history \n",
    "    epochs: number of epochs for which the model was trained\n",
    "    \"\"\"\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epoch), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, epoch), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, epoch), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, epoch), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(outpath, format='png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = os.path.join(\"data\", \"Game_of_Thrones_Script.csv\")\n",
    "epoch = 5\n",
    "batchsize = 100\n",
    "glovedim = 50\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data:\n",
    "script = pd.read_csv(inpath)\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(script['Sentence'].values, # \"Features\" (sentences)\n",
    "                                                    script['Season'].values, # Labels\n",
    "                                                    test_size = 0.15, \n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = script['Season'].values) # If full dataset has 12% sentences from season 1, have 12% of sentences in train + test\n",
    "\n",
    "# Tokenize X_train and X_test \n",
    "# (E.g. going from \"She's your new queen too.\" to [172, 8, 275, 103, 129] - each word now has an index pointing to a vocabulary list.)\n",
    "X_train_tokens, X_test_tokens, tokenizer = tokenize_X(X_train, X_test, 2500)\n",
    "\n",
    "# Find lenght of longest quote in train\n",
    "maxlen = max([len(elem) for elem in X_train]) # Maxlength to be longest element in X_train\n",
    "\n",
    "# Apply padding to X_train and X_test\n",
    "X_train, X_test = apply_padding(X_train_tokens, X_test_tokens, maxlen, \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Commencing CNN training ...\n",
      "Epoch 1/5\n",
      " 51/204 [======>.......................] - ETA: 1:00 - loss: 0.7623 - accuracy: 0.1575"
     ]
    }
   ],
   "source": [
    "# Encode labels from \"Season 8\" -> \"8\"\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Defining overall vocabulary size (adding 1 because of reserved 0 index)\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "\n",
    "# Create an embedding matrix, from glove (depending on argument)\n",
    "if glovedim == 50:\n",
    "    glove = \"glove.6B.50d.txt\"\n",
    "elif glovedim == 100:\n",
    "    glove = \"glove.6B.100d.txt\"\n",
    "\n",
    "# Creating an embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(os.path.join(\"data\", \"glove\", glove),\n",
    "                                           tokenizer.word_index, \n",
    "                                           embedding_dim)\n",
    "\n",
    "# Define model type, optimizer, layers and compile model\n",
    "model = Sequential() # Initialize sequential model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001) # Define optimizer\n",
    "# Add embedding layer\n",
    "model.add(Embedding(input_dim = vocab_size, # Vocabulary size (from Tokenizer())\n",
    "                    output_dim = embedding_dim, # Embedding dimensions as defined by argument\n",
    "                    input_length = maxlen, # Input length should be length of inputs after padding\n",
    "                    weights = [embedding_matrix], # Weights from the embedding matrix\n",
    "                    trainable = False)) # Do not train on the embeddings\n",
    "\n",
    "# CONV+ReLU -> MaxPool -> FC+ReLU -> Out\n",
    "model.add(Conv1D(8, 3, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(8, kernel_regularizer=L2(0.1), activation='relu'))\n",
    "model.add(Dense(1,  activation='softmax')) # softmax because multiclass\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy']) # Compile model (loss categorical - multiple classes)\n",
    "\n",
    "# Train model\n",
    "print(f\"[INFO] Commencing CNN training ...\")\n",
    "history = model.fit(X_train, y_train_encoded,\n",
    "                    epochs = epoch,\n",
    "                    verbose = True,\n",
    "                    validation_data = (X_test, y_test_encoded),\n",
    "                    batch_size = batchsize)\n",
    "\n",
    "\n",
    "# print plot of accuracy and loss over epochs and save it\n",
    "outpath = os.path.join(\"out\", 'cnn_training_history.png')\n",
    "plot_history(history, epoch, outpath)\n",
    "print(f\"[INFO] A plot history report has been saved succesfully: \\\"{outpath}\\\"\")\n",
    "\n",
    "# Get predictions:\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get classification report from predictions\n",
    "classif_report = pd.DataFrame(classification_report(y_test.argmax(axis=1),\n",
    "                                predictions.argmax(axis = 1),\n",
    "                                target_names = labelNames, output_dict = True))\n",
    "\n",
    "# If the folder does not already exist, create it\n",
    "if not os.path.exists(\"out\"):\n",
    "    os.makedirs(\"out\")\n",
    "\n",
    "# Printing and saving classification_report\n",
    "print(classif_report)\n",
    "classif_report_outname = os.path.join(\"out\", 'cnn_classification_report.csv')\n",
    "classif_report.to_csv(classif_report_outname, sep=',', index = True)\n",
    "print(f\"A classification report has been saved succesfully: \\\"{classif_report_outname}\\\"\")\n",
    "\n",
    "# Print\n",
    "loss, accuracy = model.evaluate(X_train_pad, y_train_encoded, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_encoded, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang101",
   "language": "python",
   "name": "lang101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
