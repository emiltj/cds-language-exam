{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out something new\n",
    "#tokens = \" \".join(list(df[\"Text\"]))\n",
    "#tokens = re.findall(r'[A-Za-z]+|\\d+|[^a-zA-Z0-9 ]+', tokens)\n",
    "\n",
    "'''\n",
    "###############################################################\n",
    "--------------- Import of modules and libraries ---------------\n",
    "###############################################################\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, string, joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from itertools import groupby\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = os.path.join(\"data\", \"grimms_fairytales.csv\")\n",
    "LSTM_layers = [128, 100]\n",
    "batchsize = 64\n",
    "epochs = 350\n",
    "ngenerated = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, sequence_length, seed_text, n_words):\n",
    "  text = []\n",
    "\n",
    "  for _ in range(n_words):\n",
    "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen = sequence_length, truncating='pre')\n",
    "\n",
    "    y_predict = model.predict_classes(encoded)\n",
    "\n",
    "    predicted_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index == y_predict:\n",
    "        predicted_word = word\n",
    "        break\n",
    "    seed_text = seed_text + ' ' + predicted_word\n",
    "    text.append(predicted_word)\n",
    "  return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "###############################################################\n",
    "------------ Defining functions to be used in main ------------\n",
    "###############################################################\n",
    "'''\n",
    "def text_processing(token_list):\n",
    "    '''\n",
    "    Function for processing a list of tokens. Removes punctuation and makes to lowercase.\n",
    "    \n",
    "    token_list: A list of tokens\n",
    "    '''\n",
    "    # Create a table for making a \"translation\" for all punctuations\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    \n",
    "    # Apply the translation\n",
    "    token_list = [token.translate(table) for token in token_list] # For token in tokens return the token after applying the translation using the table\n",
    "    \n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    token_list = [token.lower() for token in token_list if token.isalpha()]\n",
    "\n",
    "    # Return the processed token list\n",
    "    return token_list\n",
    "\n",
    "def get_sequences_running_window(token_list, window_size, window_step_length):\n",
    "    '''\n",
    "    Function for retrieving sequences of tokens in a list of tokens. Uses a moving window and saves a sequence for each window step.\n",
    "            \n",
    "    token_list: A list of tokens\n",
    "    window_size: Size of the window -> determines the length of the sequences\n",
    "    window_step_length: Step length of the window.     \n",
    "    \n",
    "    Example:\n",
    "    Input: tokenlist = [\"once\", \"upon\", \"a\", \"time\", \"in\"], window_size = 3, window_step_length = 1\n",
    "    Output: [[\"once upon a\"], [\"upon a time\"], [\"a time in\"]]\n",
    "    '''\n",
    "    # Empty list for appending the sequences for each window\n",
    "    sequences = []\n",
    "    \n",
    "    # For every number in a range of numbers\n",
    "    # e.g. [window_size, window_size+1, window_size+3, window_size+3 ...] (up until) length of token list\n",
    "    for i in range(window_size, len(token_list), window_step_length):\n",
    "        \n",
    "        # Define a sequence, which are tokens: i minus window_size to window_size. (e.g. [51-51: 51] -> [0:51])\n",
    "        sequence = token_list[i-window_size:i]\n",
    "        \n",
    "        # Join this sequence of individual tokens to a single string:\n",
    "        sequence = \" \".join(sequence)\n",
    "        \n",
    "        # Append the sequence to list of sequences\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # Return the list of sequences\n",
    "    return sequences\n",
    "\n",
    "def vectorize_sequences(sequences):\n",
    "    '''\n",
    "    Function that vectorizes sequences. The vectors correspond to an ID that is saved in the tokenizer.\n",
    "    \n",
    "    sequences: A list of sequences. E.g. [[\"once upon a\"], [\"upon a time\"], [\"a time in\"]]\n",
    "    '''\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer(filters = \"\")\n",
    "\n",
    "    # Fit tokenizer to text (having every unique word assigned an integer for the model)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "\n",
    "    # Have the sequences as integers (in a numpy array)\n",
    "    sequences_int = np.array(tokenizer.texts_to_sequences(sequences))\n",
    "    \n",
    "    # Return sequences as vectors, and the tokenizer (to be able to convert from integers back to words again)\n",
    "    return sequences_int, tokenizer\n",
    "\n",
    "def model_init(vocabulary_length, sequence_length, LSTM_layers):\n",
    "    '''\n",
    "    Function that initializes the model used in the script\n",
    "    \n",
    "    vocabulary_length: Number of unique words in the texts\n",
    "    sequence_length: Length of the sequences. Used as input shape for the model\n",
    "    LSTM_layers: Number and size of LSTM_layers. E.g. [128, 128]\n",
    "    '''\n",
    "    \n",
    "    # Initialize sequential\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add embedding layer\n",
    "    model.add(Embedding(vocabulary_length, # Input dimensions should be length of vocabulary\n",
    "                        sequence_length, # Output dimensions should be length of each of the sequences\n",
    "                        input_length = sequence_length)) # Input length, is also length of each of the sequences\n",
    "\n",
    "    # Add LSTM layers\n",
    "    for i in range(1, len(LSTM_layers)+1):\n",
    "        \n",
    "        # If the LSTM layer is not the last layer, then include \"return_sequences = True\" as input for the next layer\n",
    "        if i < len(LSTM_layers):\n",
    "            model.add(LSTM(LSTM_layers[i-1], \n",
    "                           return_sequences = True))\n",
    "            \n",
    "        # If the LSTM layer is the last, do not include return sequences.\n",
    "        if i == len(LSTM_layers):\n",
    "            model.add(LSTM(LSTM_layers[i-1]))        \n",
    "\n",
    "    # Add dense layer\n",
    "    model.add(Dense(32, \n",
    "                    activation = \"relu\"))\n",
    "\n",
    "    # Add final dense layer\n",
    "    model.add(Dense(vocabulary_length, # Output dimensions being the number of different options for predictions\n",
    "                   activation = \"softmax\")) # Use softmax to get output predictions as probabilities\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = categorical_crossentropy, optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "    \n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "def plot_history(H, epoch, outpath):\n",
    "    \"\"\"\n",
    "    Function which plots accuracy and loss over epochs (courtesy of Ross McLachlan)\n",
    "    \n",
    "    H: model history\n",
    "    epoch: Number of epochs the model was trained with\n",
    "    outpath: Outpath for saving the training history\n",
    "    \"\"\"\n",
    "    # Visualizing performance\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, format='png', dpi=100)\n",
    "\n",
    "def generate_sequences(model, tokenizer, sequence_length, seed_sequence, generated_sequence_length):\n",
    "    '''\n",
    "    Function that creates a new generated sequence using the predictions of a trained model.\n",
    "    \n",
    "    model: Model for which to use to generate sequences\n",
    "    tokenizer: A fitted tokenizer for retrieving words from integers\n",
    "    sequence_length: Length of the text sequences - to get correct format for the model\n",
    "    seed_sequence: Starting word for the model to predict from\n",
    "    generated_sequence_length: Specifies the wanted length of the generated sequence\n",
    "    '''\n",
    "    \n",
    "    # Empty list for appending the individual tokens of the generated text\n",
    "    generated_text = []\n",
    "    \n",
    "    # Go through below loop, until the wanted generated sequence length has been met\n",
    "    for _ in range(generated_sequence_length):\n",
    "        \n",
    "        # Encode the seed text as an integer - with the integer matching the index in the trained tokenizer\n",
    "        encoded = tokenizer.texts_to_sequences([seed_sequence])[0]\n",
    "        \n",
    "        # Pad the encoding to reach length of the input format of the model\n",
    "        encoded = pad_sequences([encoded], maxlen = sequence_length, truncating = \"pre\")\n",
    "        \n",
    "        # Using the encoded seed sequence that has been padded, predict the most likely next word\n",
    "        y_predicted = model.predict_classes(encoded)\n",
    "        \n",
    "        # Initialize empty string (required to make the next loop work)\n",
    "        predicted_word = \"\"\n",
    "        \n",
    "        # For each word, and its index (ID) in the dictionary of words and their ID from the trained tokenizer,\n",
    "        for word, index, in tokenizer.word_index.items():\n",
    "            \n",
    "            # If the index of the newly predicted word matches, assign the word string to \"predicted_word\"\n",
    "            if index == y_predicted:\n",
    "                predicted_word = word\n",
    "                break\n",
    "            \n",
    "        # The seed_sequence should then have added the newly predicted word, with a space in between the words for next runthrough of loop\n",
    "        seed_sequence = seed_sequence + \" \" + predicted_word\n",
    "            \n",
    "        # The generated string tokens should then be appended to the empty list\n",
    "        generated_text.append(predicted_word)\n",
    "\n",
    "    # Join together the tokens in the generated text, to have one coherent string\n",
    "    generated_string = \" \".join(generated_text)\n",
    "    \n",
    "    # Return generated string\n",
    "    return generated_string\n",
    "\n",
    "def generate_sequences_multiple(model, ngenerated, length, tokenizer, seed_sequences):\n",
    "    '''\n",
    "    Function which utilizes the generate_sequences function to generate multiple sequences of a given length using a with a random seed sequence\n",
    "    \n",
    "    model: Trained model used to generate new tokens\n",
    "    ngenerated: Number of new sequences that is wanted to be generated\n",
    "    length: Desired length of new sequences\n",
    "    tokenizer: Fitted tokenizer with token ID's\n",
    "    seed_sequences: A list of sequences that may be used as seeds\n",
    "    '''\n",
    "    \n",
    "    # Empty list of generated strings for appending to\n",
    "    generated_strings = []\n",
    "\n",
    "    # Create ngenerated (an integer) generated sequences of text\n",
    "    for _ in range(0, ngenerated):\n",
    "\n",
    "        # Take a single random sample from the possible seed sequences and join the tokens to one string\n",
    "        seed_sequence = \" \".join(sample(seed_sequences, 1))\n",
    "\n",
    "        # Generate a new string, using the seed_sequence as input\n",
    "        generated_string = generate_sequences(model = model,\n",
    "                                          tokenizer = tokenizer,\n",
    "                                          sequence_length = len(seed_sequence),\n",
    "                                          seed_sequence = seed_sequence,\n",
    "                                          generated_sequence_length = length)\n",
    "\n",
    "        # Add newly generated string to list of generated strings\n",
    "        generated_strings.append(generated_string)\n",
    "        \n",
    "    # Return generated_strings\n",
    "    return generated_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataset\n",
    "df = pd.read_csv(inpath)\n",
    "\n",
    "# Create a list of all the texts in the corpus and split it into individual tokens\n",
    "text = ' '.join(list(df[\"Text\"])).split()\n",
    "\n",
    "# Process the text and return the list of processed tokens.\n",
    "tokens = text_processing(text)\n",
    "\n",
    "# Get sequences using a running window.\n",
    "# E.g. going from [\"once upon a time in hollywood\"] to [\"once upon a\", \"upon a time\", \"a time in\", \"time in hollywood\"]\n",
    "sequences = get_sequences_running_window(tokens, 51, 1)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer to text (having every unique word assigned an integer for the model)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "\n",
    "# Have the sequences as integers (in a numpy array)\n",
    "sequences_int = np.array(tokenizer.texts_to_sequences(sequences))\n",
    "\n",
    "# Vectorize the sequences\n",
    "sequences_int, tokenizer = vectorize_sequences(sequences)\n",
    "\n",
    "# Assign X (our training variable) all the tokens in each the sequences, except for the last token in the sequence\n",
    "X = sequences_int[:, :-1]\n",
    "\n",
    "# Assign y (our testing varible) the last token in each of the sequences\n",
    "y = sequences_int[:, -1]\n",
    "\n",
    "# Define how many unique words we're dealing with\n",
    "vocabulary_length = len(set(tokens)) + 1 # +1 for the reserved index\n",
    "\n",
    "# Define the length of the training sequences\n",
    "sequence_length = X.shape[1]\n",
    "\n",
    "# Convert y to one-hot encoding instead of integer.\n",
    "y = to_categorical(y, num_classes = vocabulary_length)\n",
    "\n",
    "# Initiate model\n",
    "model = model_init(vocabulary_length, sequence_length, LSTM_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "1495/1495 [==============================] - 159s 104ms/step - loss: 6.3229 - accuracy: 0.0678\n",
      "Epoch 2/350\n",
      "1495/1495 [==============================] - 152s 101ms/step - loss: 5.6892 - accuracy: 0.0866\n",
      "Epoch 3/350\n",
      "1495/1495 [==============================] - 152s 101ms/step - loss: 5.4595 - accuracy: 0.1025\n",
      "Epoch 4/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 5.2544 - accuracy: 0.1264\n",
      "Epoch 5/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 5.0961 - accuracy: 0.1339\n",
      "Epoch 6/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 4.9626 - accuracy: 0.1428\n",
      "Epoch 7/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 4.8613 - accuracy: 0.1477\n",
      "Epoch 8/350\n",
      "1495/1495 [==============================] - 147s 99ms/step - loss: 4.7763 - accuracy: 0.1515\n",
      "Epoch 9/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 4.7065 - accuracy: 0.1544\n",
      "Epoch 10/350\n",
      "1495/1495 [==============================] - 149s 99ms/step - loss: 4.6416 - accuracy: 0.1587\n",
      "Epoch 11/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 4.6196 - accuracy: 0.1579\n",
      "Epoch 12/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 4.5549 - accuracy: 0.1614\n",
      "Epoch 13/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 4.5006 - accuracy: 0.1670\n",
      "Epoch 14/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 4.4645 - accuracy: 0.1673\n",
      "Epoch 15/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 4.4001 - accuracy: 0.1725\n",
      "Epoch 16/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 4.3827 - accuracy: 0.1726\n",
      "Epoch 17/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 4.3268 - accuracy: 0.1774\n",
      "Epoch 18/350\n",
      "1495/1495 [==============================] - 149s 99ms/step - loss: 4.2952 - accuracy: 0.1797\n",
      "Epoch 19/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 4.2486 - accuracy: 0.1826\n",
      "Epoch 20/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 4.1884 - accuracy: 0.1885\n",
      "Epoch 21/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 4.1689 - accuracy: 0.1892\n",
      "Epoch 22/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 4.1203 - accuracy: 0.1922\n",
      "Epoch 23/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 4.1012 - accuracy: 0.1922\n",
      "Epoch 24/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 4.0548 - accuracy: 0.1975\n",
      "Epoch 25/350\n",
      "1495/1495 [==============================] - 146s 98ms/step - loss: 4.0251 - accuracy: 0.1987\n",
      "Epoch 26/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.9883 - accuracy: 0.2027\n",
      "Epoch 27/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.9543 - accuracy: 0.2067\n",
      "Epoch 28/350\n",
      "1495/1495 [==============================] - 149s 99ms/step - loss: 3.9273 - accuracy: 0.2080\n",
      "Epoch 29/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.9048 - accuracy: 0.2105\n",
      "Epoch 30/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 3.8592 - accuracy: 0.2151\n",
      "Epoch 31/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.8388 - accuracy: 0.2175\n",
      "Epoch 32/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.8105 - accuracy: 0.2199\n",
      "Epoch 33/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 3.7769 - accuracy: 0.2241\n",
      "Epoch 34/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.7415 - accuracy: 0.2276\n",
      "Epoch 35/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 3.7178 - accuracy: 0.2311\n",
      "Epoch 36/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.6944 - accuracy: 0.2325\n",
      "Epoch 37/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.6518 - accuracy: 0.2387\n",
      "Epoch 38/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 3.6329 - accuracy: 0.2418\n",
      "Epoch 39/350\n",
      "1495/1495 [==============================] - 150s 101ms/step - loss: 3.6069 - accuracy: 0.2436\n",
      "Epoch 40/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 3.5801 - accuracy: 0.2477\n",
      "Epoch 41/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 3.5664 - accuracy: 0.2502\n",
      "Epoch 42/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 3.5153 - accuracy: 0.2551\n",
      "Epoch 43/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 3.4903 - accuracy: 0.2593\n",
      "Epoch 44/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 5.3365 - accuracy: 0.1328\n",
      "Epoch 45/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 5.1186 - accuracy: 0.1271\n",
      "Epoch 46/350\n",
      "1495/1495 [==============================] - 150s 101ms/step - loss: 4.9003 - accuracy: 0.1397\n",
      "Epoch 47/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 4.7330 - accuracy: 0.1463\n",
      "Epoch 48/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 4.5457 - accuracy: 0.1604\n",
      "Epoch 49/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 4.0988 - accuracy: 0.1916\n",
      "Epoch 50/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 3.9047 - accuracy: 0.2075\n",
      "Epoch 51/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 3.7654 - accuracy: 0.2234\n",
      "Epoch 52/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 3.7601 - accuracy: 0.2247\n",
      "Epoch 53/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 3.6301 - accuracy: 0.2404\n",
      "Epoch 54/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 3.5669 - accuracy: 0.2470\n",
      "Epoch 55/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 3.5236 - accuracy: 0.2527\n",
      "Epoch 56/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.4954 - accuracy: 0.2554\n",
      "Epoch 57/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.4537 - accuracy: 0.2619\n",
      "Epoch 58/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 3.4250 - accuracy: 0.2665\n",
      "Epoch 59/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.3937 - accuracy: 0.2691\n",
      "Epoch 60/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.3699 - accuracy: 0.2751\n",
      "Epoch 61/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.3366 - accuracy: 0.2771\n",
      "Epoch 62/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.3080 - accuracy: 0.2819\n",
      "Epoch 63/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.2645 - accuracy: 0.2894\n",
      "Epoch 64/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.2415 - accuracy: 0.2920\n",
      "Epoch 65/350\n",
      "1495/1495 [==============================] - 146s 98ms/step - loss: 3.2178 - accuracy: 0.2979\n",
      "Epoch 66/350\n",
      "1495/1495 [==============================] - 147s 99ms/step - loss: 3.1940 - accuracy: 0.2979\n",
      "Epoch 67/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.1666 - accuracy: 0.3034\n",
      "Epoch 68/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.1626 - accuracy: 0.3046\n",
      "Epoch 69/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.1454 - accuracy: 0.3078\n",
      "Epoch 70/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.0931 - accuracy: 0.3155\n",
      "Epoch 71/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.0630 - accuracy: 0.3172\n",
      "Epoch 72/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 3.0398 - accuracy: 0.3235\n",
      "Epoch 73/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 3.0362 - accuracy: 0.3242\n",
      "Epoch 74/350\n",
      "1495/1495 [==============================] - 146s 98ms/step - loss: 2.9974 - accuracy: 0.3283\n",
      "Epoch 75/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 2.9652 - accuracy: 0.3347\n",
      "Epoch 76/350\n",
      "1495/1495 [==============================] - 145s 97ms/step - loss: 2.9467 - accuracy: 0.3377\n",
      "Epoch 77/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495/1495 [==============================] - 148s 99ms/step - loss: 2.9273 - accuracy: 0.3414\n",
      "Epoch 78/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 2.9074 - accuracy: 0.3419\n",
      "Epoch 79/350\n",
      "1495/1495 [==============================] - 146s 98ms/step - loss: 2.8832 - accuracy: 0.3443\n",
      "Epoch 80/350\n",
      "1495/1495 [==============================] - 146s 98ms/step - loss: 2.8581 - accuracy: 0.3536\n",
      "Epoch 81/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 2.8388 - accuracy: 0.3553\n",
      "Epoch 82/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 2.8116 - accuracy: 0.3586\n",
      "Epoch 83/350\n",
      "1495/1495 [==============================] - 146s 98ms/step - loss: 2.8073 - accuracy: 0.3617\n",
      "Epoch 84/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 2.7678 - accuracy: 0.3692\n",
      "Epoch 85/350\n",
      "1495/1495 [==============================] - 146s 98ms/step - loss: 2.7541 - accuracy: 0.3713\n",
      "Epoch 86/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 2.7370 - accuracy: 0.3743\n",
      "Epoch 87/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 2.7180 - accuracy: 0.3747\n",
      "Epoch 88/350\n",
      "1495/1495 [==============================] - 156s 104ms/step - loss: 2.7118 - accuracy: 0.3797\n",
      "Epoch 89/350\n",
      "1495/1495 [==============================] - 156s 104ms/step - loss: 2.6793 - accuracy: 0.3827\n",
      "Epoch 90/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 2.6690 - accuracy: 0.3841\n",
      "Epoch 91/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.6382 - accuracy: 0.3895\n",
      "Epoch 92/350\n",
      "1495/1495 [==============================] - 156s 104ms/step - loss: 2.6151 - accuracy: 0.3942\n",
      "Epoch 93/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 2.6174 - accuracy: 0.3927\n",
      "Epoch 94/350\n",
      "1495/1495 [==============================] - 158s 105ms/step - loss: 2.5944 - accuracy: 0.3953\n",
      "Epoch 95/350\n",
      "1495/1495 [==============================] - 155s 103ms/step - loss: 2.5534 - accuracy: 0.4046\n",
      "Epoch 96/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 2.5560 - accuracy: 0.4049\n",
      "Epoch 97/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 2.5387 - accuracy: 0.4068\n",
      "Epoch 98/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.5022 - accuracy: 0.4143\n",
      "Epoch 99/350\n",
      "1495/1495 [==============================] - 153s 103ms/step - loss: 2.4931 - accuracy: 0.4142\n",
      "Epoch 100/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 2.4833 - accuracy: 0.4142\n",
      "Epoch 101/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.4719 - accuracy: 0.4173\n",
      "Epoch 102/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 2.4563 - accuracy: 0.4212\n",
      "Epoch 103/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 2.4205 - accuracy: 0.4263\n",
      "Epoch 104/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 2.4233 - accuracy: 0.4277\n",
      "Epoch 105/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 2.3906 - accuracy: 0.4346\n",
      "Epoch 106/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 2.3905 - accuracy: 0.4337\n",
      "Epoch 107/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.3623 - accuracy: 0.4390\n",
      "Epoch 108/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 2.3514 - accuracy: 0.4408\n",
      "Epoch 109/350\n",
      "1495/1495 [==============================] - 153s 103ms/step - loss: 2.3445 - accuracy: 0.4428\n",
      "Epoch 110/350\n",
      "1495/1495 [==============================] - 158s 105ms/step - loss: 2.3310 - accuracy: 0.4430\n",
      "Epoch 111/350\n",
      "1495/1495 [==============================] - 160s 107ms/step - loss: 2.3172 - accuracy: 0.4501\n",
      "Epoch 112/350\n",
      "1495/1495 [==============================] - 159s 106ms/step - loss: 2.2973 - accuracy: 0.4516\n",
      "Epoch 113/350\n",
      "1495/1495 [==============================] - 158s 105ms/step - loss: 2.2913 - accuracy: 0.4512\n",
      "Epoch 114/350\n",
      "1495/1495 [==============================] - 155s 103ms/step - loss: 2.2728 - accuracy: 0.4542\n",
      "Epoch 115/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 2.2557 - accuracy: 0.4607\n",
      "Epoch 116/350\n",
      "1495/1495 [==============================] - 152s 101ms/step - loss: 2.2754 - accuracy: 0.4555\n",
      "Epoch 117/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 2.2427 - accuracy: 0.4620\n",
      "Epoch 118/350\n",
      "1495/1495 [==============================] - 150s 101ms/step - loss: 2.2149 - accuracy: 0.4680\n",
      "Epoch 119/350\n",
      "1495/1495 [==============================] - 153s 103ms/step - loss: 2.2145 - accuracy: 0.4660\n",
      "Epoch 120/350\n",
      "1495/1495 [==============================] - 152s 101ms/step - loss: 2.2097 - accuracy: 0.4665\n",
      "Epoch 121/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 2.1876 - accuracy: 0.4737\n",
      "Epoch 122/350\n",
      "1495/1495 [==============================] - 156s 104ms/step - loss: 2.1661 - accuracy: 0.4746\n",
      "Epoch 123/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 2.1515 - accuracy: 0.4793\n",
      "Epoch 124/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 2.1460 - accuracy: 0.4832\n",
      "Epoch 125/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 2.1349 - accuracy: 0.4819\n",
      "Epoch 126/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.1203 - accuracy: 0.4850\n",
      "Epoch 127/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 2.1540 - accuracy: 0.4784\n",
      "Epoch 128/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 2.1050 - accuracy: 0.4885\n",
      "Epoch 129/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 2.0924 - accuracy: 0.4910\n",
      "Epoch 130/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 2.0901 - accuracy: 0.4923\n",
      "Epoch 131/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 2.0756 - accuracy: 0.4947\n",
      "Epoch 132/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 2.0618 - accuracy: 0.4958\n",
      "Epoch 133/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 2.0531 - accuracy: 0.4957\n",
      "Epoch 134/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.0482 - accuracy: 0.4990\n",
      "Epoch 135/350\n",
      "1495/1495 [==============================] - 157s 105ms/step - loss: 2.0280 - accuracy: 0.5025\n",
      "Epoch 136/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.0080 - accuracy: 0.5067\n",
      "Epoch 137/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 2.0022 - accuracy: 0.5080\n",
      "Epoch 138/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.0205 - accuracy: 0.5058\n",
      "Epoch 139/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 2.0006 - accuracy: 0.5058\n",
      "Epoch 140/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.9892 - accuracy: 0.5111\n",
      "Epoch 141/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.9889 - accuracy: 0.5087\n",
      "Epoch 142/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 2.0361 - accuracy: 0.5016\n",
      "Epoch 143/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.9513 - accuracy: 0.5186\n",
      "Epoch 144/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.9573 - accuracy: 0.5158\n",
      "Epoch 145/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.9368 - accuracy: 0.5241\n",
      "Epoch 146/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.9263 - accuracy: 0.5224\n",
      "Epoch 147/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.9203 - accuracy: 0.5245\n",
      "Epoch 148/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.9331 - accuracy: 0.5192\n",
      "Epoch 149/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.9121 - accuracy: 0.5250\n",
      "Epoch 150/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.8981 - accuracy: 0.5284\n",
      "Epoch 151/350\n",
      "1495/1495 [==============================] - 150s 101ms/step - loss: 1.8830 - accuracy: 0.5302\n",
      "Epoch 152/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.8807 - accuracy: 0.5315\n",
      "Epoch 153/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.8797 - accuracy: 0.5305\n",
      "Epoch 154/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.8749 - accuracy: 0.5326\n",
      "Epoch 155/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.9086 - accuracy: 0.5248\n",
      "Epoch 156/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.8478 - accuracy: 0.5365\n",
      "Epoch 157/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.8514 - accuracy: 0.5377\n",
      "Epoch 158/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.8433 - accuracy: 0.5403\n",
      "Epoch 159/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.8159 - accuracy: 0.5473\n",
      "Epoch 160/350\n",
      "1495/1495 [==============================] - 150s 101ms/step - loss: 1.9011 - accuracy: 0.5268\n",
      "Epoch 161/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.8193 - accuracy: 0.5445\n",
      "Epoch 162/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.7968 - accuracy: 0.5489\n",
      "Epoch 163/350\n",
      "1495/1495 [==============================] - 149s 99ms/step - loss: 1.8056 - accuracy: 0.5488\n",
      "Epoch 164/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.7996 - accuracy: 0.5495\n",
      "Epoch 165/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.8198 - accuracy: 0.5424\n",
      "Epoch 166/350\n",
      "1495/1495 [==============================] - 149s 99ms/step - loss: 1.7882 - accuracy: 0.5489\n",
      "Epoch 167/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 1.7554 - accuracy: 0.5593\n",
      "Epoch 168/350\n",
      "1495/1495 [==============================] - 153s 103ms/step - loss: 1.7874 - accuracy: 0.5519\n",
      "Epoch 169/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.7832 - accuracy: 0.5531\n",
      "Epoch 170/350\n",
      "1495/1495 [==============================] - 149s 99ms/step - loss: 1.7569 - accuracy: 0.5598\n",
      "Epoch 171/350\n",
      "1495/1495 [==============================] - 147s 99ms/step - loss: 1.7665 - accuracy: 0.5547\n",
      "Epoch 172/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.7575 - accuracy: 0.5566\n",
      "Epoch 173/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.7338 - accuracy: 0.5638\n",
      "Epoch 174/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.7366 - accuracy: 0.5594\n",
      "Epoch 175/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.7499 - accuracy: 0.5610\n",
      "Epoch 176/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.7230 - accuracy: 0.5637\n",
      "Epoch 177/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 1.7234 - accuracy: 0.5657\n",
      "Epoch 178/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.7310 - accuracy: 0.5606\n",
      "Epoch 179/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.7065 - accuracy: 0.5680\n",
      "Epoch 180/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.7028 - accuracy: 0.5685\n",
      "Epoch 181/350\n",
      "1495/1495 [==============================] - 149s 99ms/step - loss: 1.7087 - accuracy: 0.5665\n",
      "Epoch 182/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.7072 - accuracy: 0.5656\n",
      "Epoch 183/350\n",
      "1495/1495 [==============================] - 147s 99ms/step - loss: 1.6835 - accuracy: 0.5745\n",
      "Epoch 184/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6808 - accuracy: 0.5714\n",
      "Epoch 185/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.6960 - accuracy: 0.5701\n",
      "Epoch 186/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6706 - accuracy: 0.5747\n",
      "Epoch 187/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6864 - accuracy: 0.5726\n",
      "Epoch 188/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6523 - accuracy: 0.5803\n",
      "Epoch 189/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6623 - accuracy: 0.5767\n",
      "Epoch 190/350\n",
      "1495/1495 [==============================] - 147s 99ms/step - loss: 1.6415 - accuracy: 0.5800\n",
      "Epoch 191/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6556 - accuracy: 0.5799\n",
      "Epoch 192/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6536 - accuracy: 0.5799\n",
      "Epoch 193/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.6505 - accuracy: 0.5802\n",
      "Epoch 194/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6576 - accuracy: 0.5786\n",
      "Epoch 195/350\n",
      "1495/1495 [==============================] - 149s 99ms/step - loss: 1.6111 - accuracy: 0.5890\n",
      "Epoch 196/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.6206 - accuracy: 0.5878\n",
      "Epoch 197/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6573 - accuracy: 0.5770\n",
      "Epoch 198/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6003 - accuracy: 0.5909\n",
      "Epoch 199/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6180 - accuracy: 0.5851\n",
      "Epoch 200/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.6178 - accuracy: 0.5849\n",
      "Epoch 201/350\n",
      "1495/1495 [==============================] - 154s 103ms/step - loss: 1.6448 - accuracy: 0.5791\n",
      "Epoch 202/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.6434 - accuracy: 0.5811\n",
      "Epoch 203/350\n",
      "1495/1495 [==============================] - 161s 108ms/step - loss: 1.6210 - accuracy: 0.5854\n",
      "Epoch 204/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.6030 - accuracy: 0.5916\n",
      "Epoch 205/350\n",
      "1495/1495 [==============================] - 159s 106ms/step - loss: 1.6384 - accuracy: 0.5816\n",
      "Epoch 206/350\n",
      "1495/1495 [==============================] - 177s 118ms/step - loss: 1.5804 - accuracy: 0.5952\n",
      "Epoch 207/350\n",
      "1495/1495 [==============================] - 164s 110ms/step - loss: 1.5642 - accuracy: 0.5998\n",
      "Epoch 208/350\n",
      "1495/1495 [==============================] - 163s 109ms/step - loss: 1.6314 - accuracy: 0.5813\n",
      "Epoch 209/350\n",
      "1495/1495 [==============================] - 158s 106ms/step - loss: 1.5803 - accuracy: 0.5930\n",
      "Epoch 210/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5577 - accuracy: 0.5974\n",
      "Epoch 211/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5718 - accuracy: 0.5943\n",
      "Epoch 212/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5870 - accuracy: 0.5947\n",
      "Epoch 213/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5616 - accuracy: 0.5982\n",
      "Epoch 214/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5613 - accuracy: 0.5991\n",
      "Epoch 215/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.5504 - accuracy: 0.5993\n",
      "Epoch 216/350\n",
      "1495/1495 [==============================] - 150s 101ms/step - loss: 1.5301 - accuracy: 0.6062\n",
      "Epoch 217/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.5585 - accuracy: 0.5965\n",
      "Epoch 218/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.5311 - accuracy: 0.6023\n",
      "Epoch 219/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.5448 - accuracy: 0.6019\n",
      "Epoch 220/350\n",
      "1495/1495 [==============================] - 153s 103ms/step - loss: 1.5360 - accuracy: 0.6046\n",
      "Epoch 221/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5606 - accuracy: 0.5969\n",
      "Epoch 222/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5235 - accuracy: 0.6050\n",
      "Epoch 223/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.5212 - accuracy: 0.6064\n",
      "Epoch 224/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.5264 - accuracy: 0.6069\n",
      "Epoch 225/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5156 - accuracy: 0.6075\n",
      "Epoch 226/350\n",
      "1495/1495 [==============================] - 150s 101ms/step - loss: 1.5141 - accuracy: 0.6085\n",
      "Epoch 227/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495/1495 [==============================] - 152s 101ms/step - loss: 1.5174 - accuracy: 0.6070\n",
      "Epoch 228/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.5016 - accuracy: 0.6112\n",
      "Epoch 229/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 1.5093 - accuracy: 0.6097\n",
      "Epoch 230/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.5157 - accuracy: 0.6068\n",
      "Epoch 231/350\n",
      "1495/1495 [==============================] - 147s 98ms/step - loss: 1.4768 - accuracy: 0.6168\n",
      "Epoch 232/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.4885 - accuracy: 0.6163\n",
      "Epoch 233/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.5062 - accuracy: 0.6082\n",
      "Epoch 234/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.4680 - accuracy: 0.6171\n",
      "Epoch 235/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.4798 - accuracy: 0.6150\n",
      "Epoch 236/350\n",
      "1495/1495 [==============================] - 148s 99ms/step - loss: 1.4882 - accuracy: 0.6159\n",
      "Epoch 237/350\n",
      "1495/1495 [==============================] - 156s 104ms/step - loss: 1.4987 - accuracy: 0.6142\n",
      "Epoch 238/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.4865 - accuracy: 0.6155\n",
      "Epoch 239/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.4681 - accuracy: 0.6171\n",
      "Epoch 240/350\n",
      "1495/1495 [==============================] - 150s 100ms/step - loss: 1.4836 - accuracy: 0.6139\n",
      "Epoch 241/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.4612 - accuracy: 0.6206\n",
      "Epoch 242/350\n",
      "1495/1495 [==============================] - 161s 108ms/step - loss: 1.4691 - accuracy: 0.6169\n",
      "Epoch 243/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.4583 - accuracy: 0.6202\n",
      "Epoch 244/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 1.4494 - accuracy: 0.6223\n",
      "Epoch 245/350\n",
      "1495/1495 [==============================] - 149s 100ms/step - loss: 1.4602 - accuracy: 0.6204\n",
      "Epoch 246/350\n",
      "1495/1495 [==============================] - 169s 113ms/step - loss: 1.4445 - accuracy: 0.6215\n",
      "Epoch 247/350\n",
      "1495/1495 [==============================] - 202s 135ms/step - loss: 1.4566 - accuracy: 0.6174\n",
      "Epoch 248/350\n",
      "1495/1495 [==============================] - 180s 121ms/step - loss: 1.4620 - accuracy: 0.6185\n",
      "Epoch 249/350\n",
      "1495/1495 [==============================] - 169s 113ms/step - loss: 1.4599 - accuracy: 0.6186\n",
      "Epoch 250/350\n",
      "1495/1495 [==============================] - 206s 138ms/step - loss: 1.4850 - accuracy: 0.6140\n",
      "Epoch 251/350\n",
      "1495/1495 [==============================] - 218s 146ms/step - loss: 1.4336 - accuracy: 0.6254\n",
      "Epoch 252/350\n",
      "1495/1495 [==============================] - 220s 147ms/step - loss: 1.4476 - accuracy: 0.6224\n",
      "Epoch 253/350\n",
      "1495/1495 [==============================] - 220s 147ms/step - loss: 1.4502 - accuracy: 0.6226\n",
      "Epoch 254/350\n",
      "1495/1495 [==============================] - 241s 161ms/step - loss: 1.4313 - accuracy: 0.6263\n",
      "Epoch 255/350\n",
      "1495/1495 [==============================] - 242s 162ms/step - loss: 1.4367 - accuracy: 0.6246\n",
      "Epoch 256/350\n",
      "1495/1495 [==============================] - 243s 163ms/step - loss: 1.4307 - accuracy: 0.6253\n",
      "Epoch 257/350\n",
      "1495/1495 [==============================] - 221s 148ms/step - loss: 1.4333 - accuracy: 0.6255\n",
      "Epoch 258/350\n",
      "1495/1495 [==============================] - 222s 148ms/step - loss: 1.4083 - accuracy: 0.6321\n",
      "Epoch 259/350\n",
      "1495/1495 [==============================] - 218s 146ms/step - loss: 1.4357 - accuracy: 0.6259\n",
      "Epoch 260/350\n",
      "1495/1495 [==============================] - 220s 147ms/step - loss: 1.4251 - accuracy: 0.6278\n",
      "Epoch 261/350\n",
      "1495/1495 [==============================] - 219s 147ms/step - loss: 1.4560 - accuracy: 0.6193\n",
      "Epoch 262/350\n",
      "1495/1495 [==============================] - 226s 151ms/step - loss: 1.4331 - accuracy: 0.6258\n",
      "Epoch 263/350\n",
      "1495/1495 [==============================] - 212s 142ms/step - loss: 1.4592 - accuracy: 0.6203\n",
      "Epoch 264/350\n",
      "1495/1495 [==============================] - 213s 143ms/step - loss: 1.3955 - accuracy: 0.6353\n",
      "Epoch 265/350\n",
      "1495/1495 [==============================] - 189s 126ms/step - loss: 1.4366 - accuracy: 0.6243\n",
      "Epoch 266/350\n",
      "1495/1495 [==============================] - 165s 110ms/step - loss: 1.3930 - accuracy: 0.6316\n",
      "Epoch 267/350\n",
      "1495/1495 [==============================] - 195s 131ms/step - loss: 1.4066 - accuracy: 0.6327\n",
      "Epoch 268/350\n",
      "1495/1495 [==============================] - 164s 109ms/step - loss: 1.4076 - accuracy: 0.6323\n",
      "Epoch 269/350\n",
      "1495/1495 [==============================] - 163s 109ms/step - loss: 1.4504 - accuracy: 0.6215\n",
      "Epoch 270/350\n",
      "1495/1495 [==============================] - 163s 109ms/step - loss: 1.3891 - accuracy: 0.6320\n",
      "Epoch 271/350\n",
      "1495/1495 [==============================] - 162s 109ms/step - loss: 1.3884 - accuracy: 0.6371\n",
      "Epoch 272/350\n",
      "1495/1495 [==============================] - 165s 111ms/step - loss: 1.3743 - accuracy: 0.6385\n",
      "Epoch 273/350\n",
      "1495/1495 [==============================] - 157s 105ms/step - loss: 1.4160 - accuracy: 0.6287\n",
      "Epoch 274/350\n",
      "1495/1495 [==============================] - 161s 108ms/step - loss: 1.4073 - accuracy: 0.6298\n",
      "Epoch 275/350\n",
      "1495/1495 [==============================] - 158s 105ms/step - loss: 1.3805 - accuracy: 0.6377\n",
      "Epoch 276/350\n",
      "1495/1495 [==============================] - 159s 106ms/step - loss: 1.3765 - accuracy: 0.6386\n",
      "Epoch 277/350\n",
      "1495/1495 [==============================] - 166s 111ms/step - loss: 1.3877 - accuracy: 0.6360\n",
      "Epoch 278/350\n",
      "1495/1495 [==============================] - 159s 106ms/step - loss: 1.3955 - accuracy: 0.6332\n",
      "Epoch 279/350\n",
      "1495/1495 [==============================] - 161s 108ms/step - loss: 1.4084 - accuracy: 0.6292\n",
      "Epoch 280/350\n",
      "1495/1495 [==============================] - 161s 108ms/step - loss: 1.3763 - accuracy: 0.6371\n",
      "Epoch 281/350\n",
      "1495/1495 [==============================] - 156s 105ms/step - loss: 1.3886 - accuracy: 0.6339\n",
      "Epoch 282/350\n",
      "1495/1495 [==============================] - 191s 127ms/step - loss: 1.3787 - accuracy: 0.6364\n",
      "Epoch 283/350\n",
      "1495/1495 [==============================] - 169s 113ms/step - loss: 1.3697 - accuracy: 0.6398\n",
      "Epoch 284/350\n",
      "1495/1495 [==============================] - 164s 109ms/step - loss: 1.3665 - accuracy: 0.6390\n",
      "Epoch 285/350\n",
      "1495/1495 [==============================] - 157s 105ms/step - loss: 1.3710 - accuracy: 0.6378\n",
      "Epoch 286/350\n",
      "1495/1495 [==============================] - 186s 124ms/step - loss: 1.3716 - accuracy: 0.6385\n",
      "Epoch 287/350\n",
      "1495/1495 [==============================] - 159s 106ms/step - loss: 1.3563 - accuracy: 0.6411\n",
      "Epoch 288/350\n",
      "1495/1495 [==============================] - 158s 105ms/step - loss: 1.3564 - accuracy: 0.6439\n",
      "Epoch 289/350\n",
      "1495/1495 [==============================] - 161s 108ms/step - loss: 1.3642 - accuracy: 0.6417\n",
      "Epoch 290/350\n",
      "1495/1495 [==============================] - 163s 109ms/step - loss: 1.3513 - accuracy: 0.6446\n",
      "Epoch 291/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.3764 - accuracy: 0.6374\n",
      "Epoch 292/350\n",
      "1495/1495 [==============================] - 152s 101ms/step - loss: 1.3463 - accuracy: 0.6446\n",
      "Epoch 293/350\n",
      "1495/1495 [==============================] - 155s 104ms/step - loss: 1.3489 - accuracy: 0.6434\n",
      "Epoch 294/350\n",
      "1495/1495 [==============================] - 157s 105ms/step - loss: 1.3360 - accuracy: 0.6470\n",
      "Epoch 295/350\n",
      "1495/1495 [==============================] - 169s 113ms/step - loss: 1.3385 - accuracy: 0.6457\n",
      "Epoch 296/350\n",
      "1495/1495 [==============================] - 168s 112ms/step - loss: 1.3548 - accuracy: 0.6428\n",
      "Epoch 297/350\n",
      "1495/1495 [==============================] - 168s 112ms/step - loss: 1.3568 - accuracy: 0.6440\n",
      "Epoch 298/350\n",
      "1495/1495 [==============================] - 164s 109ms/step - loss: 1.3544 - accuracy: 0.6444\n",
      "Epoch 299/350\n",
      "1495/1495 [==============================] - 163s 109ms/step - loss: 1.3381 - accuracy: 0.6463\n",
      "Epoch 300/350\n",
      "1495/1495 [==============================] - 158s 106ms/step - loss: 1.3368 - accuracy: 0.6458\n",
      "Epoch 301/350\n",
      "1495/1495 [==============================] - 159s 106ms/step - loss: 1.3353 - accuracy: 0.6466\n",
      "Epoch 302/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495/1495 [==============================] - 159s 106ms/step - loss: 1.3523 - accuracy: 0.6404\n",
      "Epoch 303/350\n",
      "1495/1495 [==============================] - 162s 108ms/step - loss: 1.3530 - accuracy: 0.6414\n",
      "Epoch 304/350\n",
      "1495/1495 [==============================] - 163s 109ms/step - loss: 1.3353 - accuracy: 0.6460\n",
      "Epoch 305/350\n",
      "1495/1495 [==============================] - 160s 107ms/step - loss: 1.3165 - accuracy: 0.6544\n",
      "Epoch 306/350\n",
      "1495/1495 [==============================] - 162s 108ms/step - loss: 1.3328 - accuracy: 0.6456\n",
      "Epoch 307/350\n",
      "1495/1495 [==============================] - 162s 108ms/step - loss: 1.3724 - accuracy: 0.6377\n",
      "Epoch 308/350\n",
      "1495/1495 [==============================] - 180s 120ms/step - loss: 1.3191 - accuracy: 0.6514\n",
      "Epoch 309/350\n",
      "1495/1495 [==============================] - 179s 120ms/step - loss: 1.3693 - accuracy: 0.6369\n",
      "Epoch 310/350\n",
      "1495/1495 [==============================] - 157s 105ms/step - loss: 1.2975 - accuracy: 0.6557\n",
      "Epoch 311/350\n",
      "1495/1495 [==============================] - 171s 115ms/step - loss: 1.3965 - accuracy: 0.6311\n",
      "Epoch 312/350\n",
      "1495/1495 [==============================] - 164s 110ms/step - loss: 1.2850 - accuracy: 0.6597\n",
      "Epoch 313/350\n",
      "1495/1495 [==============================] - 176s 118ms/step - loss: 1.2899 - accuracy: 0.6558\n",
      "Epoch 314/350\n",
      "1495/1495 [==============================] - 171s 115ms/step - loss: 1.3293 - accuracy: 0.6481\n",
      "Epoch 315/350\n",
      "1495/1495 [==============================] - 175s 117ms/step - loss: 1.3382 - accuracy: 0.6462\n",
      "Epoch 316/350\n",
      "1495/1495 [==============================] - 175s 117ms/step - loss: 1.3203 - accuracy: 0.6503\n",
      "Epoch 317/350\n",
      "1495/1495 [==============================] - 163s 109ms/step - loss: 1.3986 - accuracy: 0.6310\n",
      "Epoch 318/350\n",
      "1495/1495 [==============================] - 197s 132ms/step - loss: 1.2950 - accuracy: 0.6549\n",
      "Epoch 319/350\n",
      "1495/1495 [==============================] - 202s 135ms/step - loss: 1.2842 - accuracy: 0.6600\n",
      "Epoch 320/350\n",
      "1495/1495 [==============================] - 171s 115ms/step - loss: 1.3038 - accuracy: 0.6532\n",
      "Epoch 321/350\n",
      "1495/1495 [==============================] - 175s 117ms/step - loss: 1.2995 - accuracy: 0.6567\n",
      "Epoch 322/350\n",
      "1495/1495 [==============================] - 172s 115ms/step - loss: 1.2968 - accuracy: 0.6543\n",
      "Epoch 323/350\n",
      "1495/1495 [==============================] - 159s 106ms/step - loss: 1.3053 - accuracy: 0.6532\n",
      "Epoch 324/350\n",
      "1495/1495 [==============================] - 155s 103ms/step - loss: 1.2899 - accuracy: 0.6569\n",
      "Epoch 325/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 1.2798 - accuracy: 0.6607\n",
      "Epoch 326/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 1.3385 - accuracy: 0.6456\n",
      "Epoch 327/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.3324 - accuracy: 0.6454\n",
      "Epoch 328/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.2717 - accuracy: 0.6626\n",
      "Epoch 329/350\n",
      "1495/1495 [==============================] - 151s 101ms/step - loss: 1.2879 - accuracy: 0.6566\n",
      "Epoch 330/350\n",
      "1495/1495 [==============================] - 153s 102ms/step - loss: 1.3098 - accuracy: 0.6513\n",
      "Epoch 331/350\n",
      "1495/1495 [==============================] - 152s 102ms/step - loss: 1.3381 - accuracy: 0.6431\n",
      "Epoch 332/350\n",
      "1495/1495 [==============================] - 169s 113ms/step - loss: 1.2772 - accuracy: 0.6593\n",
      "Epoch 333/350\n",
      "1495/1495 [==============================] - 174s 116ms/step - loss: 1.3125 - accuracy: 0.6522\n",
      "Epoch 334/350\n",
      "1495/1495 [==============================] - 191s 128ms/step - loss: 1.2972 - accuracy: 0.6552\n",
      "Epoch 335/350\n",
      "1495/1495 [==============================] - 176s 118ms/step - loss: 1.2703 - accuracy: 0.6628\n",
      "Epoch 336/350\n",
      "1495/1495 [==============================] - 172s 115ms/step - loss: 1.2870 - accuracy: 0.6572\n",
      "Epoch 337/350\n",
      "1495/1495 [==============================] - 171s 115ms/step - loss: 1.2918 - accuracy: 0.6573\n",
      "Epoch 338/350\n",
      "1495/1495 [==============================] - 165s 111ms/step - loss: 1.2939 - accuracy: 0.6582\n",
      "Epoch 339/350\n",
      "1495/1495 [==============================] - 159s 107ms/step - loss: 1.2844 - accuracy: 0.6595\n",
      "Epoch 340/350\n",
      "1495/1495 [==============================] - 156s 104ms/step - loss: 1.2705 - accuracy: 0.6614\n",
      "Epoch 341/350\n",
      "1495/1495 [==============================] - 176s 117ms/step - loss: 1.2922 - accuracy: 0.6564\n",
      "Epoch 342/350\n",
      "1495/1495 [==============================] - 171s 115ms/step - loss: 1.2835 - accuracy: 0.6567\n",
      "Epoch 343/350\n",
      "1495/1495 [==============================] - 175s 117ms/step - loss: 1.2702 - accuracy: 0.6606\n",
      "Epoch 344/350\n",
      "1495/1495 [==============================] - 172s 115ms/step - loss: 1.3521 - accuracy: 0.6412\n",
      "Epoch 345/350\n",
      "1495/1495 [==============================] - 175s 117ms/step - loss: 1.2429 - accuracy: 0.6681\n",
      "Epoch 346/350\n",
      "1495/1495 [==============================] - 168s 113ms/step - loss: 1.2953 - accuracy: 0.6520\n",
      "Epoch 347/350\n",
      "1495/1495 [==============================] - 181s 121ms/step - loss: 1.2831 - accuracy: 0.6585\n",
      "Epoch 348/350\n",
      "1495/1495 [==============================] - 189s 127ms/step - loss: 1.2647 - accuracy: 0.6620\n",
      "Epoch 349/350\n",
      "1495/1495 [==============================] - 199s 133ms/step - loss: 1.2697 - accuracy: 0.6596\n",
      "Epoch 350/350\n",
      "1495/1495 [==============================] - 222s 149ms/step - loss: 1.3853 - accuracy: 0.6350\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the data\n",
    "history = model.fit(X, y, batch_size = batchsize, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle weakref objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f4a2de01336a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Saving the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutpath_cnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnn_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutpath_cnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] The trained CNN model has been saved: \\\"{outpath_cnn_model}\\\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cds-language-exam/lang101/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cds-language-exam/lang101/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cds-language-exam/lang101/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cds-language-exam/lang101/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cds-language-exam/lang101/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cds-language-exam/lang101/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    844\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cds-language-exam/lang101/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce_ex__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle weakref objects"
     ]
    }
   ],
   "source": [
    "# Saving the trained model\n",
    "outpath_cnn_model = os.path.join(\"out\", \"cnn_model.pkl\")\n",
    "joblib.dump(model, outpath_cnn_model)\n",
    "print(f\"[INFO] The trained CNN model has been saved: \\\"{outpath_cnn_model}\\\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outpath_training_hist = os.path.join(\"out\", \"training_hist.png\")\n",
    "plot_history(history, epochs, outpath_training_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "###############################################################\n",
    "---------- Defining the main function of the script -----------\n",
    "###############################################################\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cds-au604547/cds-language-exam/lang101/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the enchantress a whit the leathern they had and snowdrop went to the son he equipped from long the bear'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_seq(model, tokenizer, sequence_length, sequences[2], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fox came to the sea some beasts and said to himself i will let it choose safe hazeltree where the'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequences(model, tokenizer, sequence_length, sequences[2189], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences_10 = generate_sequences_multiple(model, 50, 10, tokenizer, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences_30 = generate_sequences_multiple(model, 50, 30, tokenizer, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences_50 = generate_sequences_multiple(model, 50, 50, tokenizer, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences_100 = generate_sequences_multiple(model, 50, 100, tokenizer, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({\"length_10\" : generated_sequences_10,\n",
    "                             \"length_30\" : generated_sequences_30,\n",
    "                             \"length_50\" : generated_sequences_30,\n",
    "                             \"length_100\" : generated_sequences_30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outpath and save dataframe\n",
    "if not os.path.exists(\"out\"): # If the folder does not already exist, create it\n",
    "    os.makedirs(\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = os.path.join(\"out\", \"generated_strings.csv\")\n",
    "df.to_csv(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_10</th>\n",
       "      <th>length_30</th>\n",
       "      <th>length_50</th>\n",
       "      <th>length_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from whence his wealth came to see them to the</td>\n",
       "      <td>soon as he had heard a wishingcloak was a loud...</td>\n",
       "      <td>soon as he had heard a wishingcloak was a loud...</td>\n",
       "      <td>soon as he had heard a wishingcloak was a loud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cried is himself give me my head then the king</td>\n",
       "      <td>fast clack light upon a little of his four bro...</td>\n",
       "      <td>fast clack light upon a little of his four bro...</td>\n",
       "      <td>fast clack light upon a little of his four bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitchen and shook out the willowwren flew up t...</td>\n",
       "      <td>i shall not be of and as if i have you said he...</td>\n",
       "      <td>i shall not be of and as if i have you said he...</td>\n",
       "      <td>i shall not be of and as if i have you said he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>foot and limped hobblety jib hobblety and when...</td>\n",
       "      <td>her hundred different long years in the frogs ...</td>\n",
       "      <td>her hundred different long years in the frogs ...</td>\n",
       "      <td>her hundred different long years in the frogs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and when he had gone on her she walked out</td>\n",
       "      <td>here but chanticleer began into his pocket the...</td>\n",
       "      <td>here but chanticleer began into his pocket the...</td>\n",
       "      <td>here but chanticleer began into his pocket the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           length_10  \\\n",
       "0     from whence his wealth came to see them to the   \n",
       "1     cried is himself give me my head then the king   \n",
       "2  kitchen and shook out the willowwren flew up t...   \n",
       "3  foot and limped hobblety jib hobblety and when...   \n",
       "4         and when he had gone on her she walked out   \n",
       "\n",
       "                                           length_30  \\\n",
       "0  soon as he had heard a wishingcloak was a loud...   \n",
       "1  fast clack light upon a little of his four bro...   \n",
       "2  i shall not be of and as if i have you said he...   \n",
       "3  her hundred different long years in the frogs ...   \n",
       "4  here but chanticleer began into his pocket the...   \n",
       "\n",
       "                                           length_50  \\\n",
       "0  soon as he had heard a wishingcloak was a loud...   \n",
       "1  fast clack light upon a little of his four bro...   \n",
       "2  i shall not be of and as if i have you said he...   \n",
       "3  her hundred different long years in the frogs ...   \n",
       "4  here but chanticleer began into his pocket the...   \n",
       "\n",
       "                                          length_100  \n",
       "0  soon as he had heard a wishingcloak was a loud...  \n",
       "1  fast clack light upon a little of his four bro...  \n",
       "2  i shall not be of and as if i have you said he...  \n",
       "3  her hundred different long years in the frogs ...  \n",
       "4  here but chanticleer began into his pocket the...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang101",
   "language": "python",
   "name": "lang101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
